{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis \n",
    "\n",
    "_August 24, 2020_\n",
    "\n",
    "Agenda today:\n",
    "- dimensionality reduction: motivation for PCA & intuition for PCA\n",
    "- Construct Principal Components using singular value decomposition and interpret them \n",
    "- Example implementation in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I. Motivation and Intuition for PCA\n",
    "We can easily visualize data with two or three dimensions, but as the dimensions of our data increase, making sense of our data through visualization becomes challenging. One of such solution is pairplots:\n",
    "<img src=\"attachment:Screen%20Shot%202019-03-24%20at%207.02.22%20PM.png\" style=\"width:500px;\">\n",
    "\n",
    "Pairplots are a great way of visualizing the pairwise correlations between variables, so are heatmaps. However, as the dimensions increase to hundreds or even thousands, using pairplots or heatmap might not be suitable anymore. In addition, pairplots and heatmap only show pairwise correlations between variables, but not linear combinations of variables. Therefore, an algorithm that perform dimensionality reduction is needed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II. Constructing PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is PCA?\n",
    "PCA is a method to summarize our data -- it essentially construct new characteristics from the original dataset and summarize our original variables. In other words, unlike other dimensionality reduction algorithm that retain or select the original features, PCA linearly transform the original variables. PCA extracts our features and output principal components that are linear combinations of the original variables by projecting the original variables to lower dimensions in such a way that it captures the variability of the data. PCA would not discard any information or any variables, and it is up to us to select the features we want to include in our model. PCA outputs $p$ principal components where $p$ equals the number of features we have in our dataset. \n",
    "\n",
    "#### What is principal components?\n",
    "The principal components are linear combination of the original variables. The first principal component represents the combination of variables that explain the most amount of variances in the data, and the second component explains the second most amount of variance in the data, and so on. For example, \n",
    "<img src=\"attachment:Screen%20Shot%202019-03-25%20at%2011.18.58%20AM.png\" style=\"width:500px;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA Step-by-step\n",
    "1. It is important to **center** and standardize your data. PCA lives off of correlation and covariance of your data, and using wildly different scales could lead to inflated weights for the linear combination. Let's call this centered and standardized matrix **Z**. \n",
    "2. Calculate a covariance matrix of p x p where p responds to number of predictors. \n",
    "3. Calculate the eigenvectors and eigenvalues of the covariance matrix. \n",
    "4. Arrange the eigenvalues from largest to smallest. You should obtain p eigenvalues which correspond to number of components. \n",
    "5. Choose the amount of components you want to include based on number of variance explained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA Terminology \n",
    "- Eigenvectors \n",
    "    - Eigenvectors are the direction of the unit scaled vector in the p-dimensional space for the principal components. \n",
    "- Eigenvalues\n",
    "    - Eigenvalues are the magnitude of the variation in each of the components, denoted as $\\lambda$\n",
    "- Loadings\n",
    "    - Shall not be confused with eigenvectors\n",
    "    - $Loadings=Eigenvectorsâ‹…\\sqrt{Eigenvalues}$\n",
    "    - loadings are the covariances/correlations between the original variables and the unit-scaled components\n",
    "    - Properties of Loadings:\n",
    "        - Their sums of squares within each component are the eigenvalues (components' variances).\n",
    "        - Loadings are coefficients in linear combination describing a variable by the (standardized) components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part III. PCA Example\n",
    "In this example, we want to examine how to reduce the dimensionality of the diamonds dataset, and which features produce the highest variability in our dataset. Let's find out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "df = sns.load_dataset('mpg')\n",
    "df.head()\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(axis = 0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = df[df.horsepower != '?']\n",
    "df['horsepower'] = df['horsepower'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['cylinders', 'displacement', 'horsepower', 'weight',\n",
    "       'acceleration','horsepower']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the data \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Separating out the features\n",
    "x = df.loc[:, features].values\n",
    "# Separating out the target\n",
    "y = df.loc[:,['mpg']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizing the features\n",
    "x = StandardScaler().fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create principal components \n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()\n",
    "principalComponents = pca.fit_transform(x)\n",
    "principalDf = pd.DataFrame(data = principalComponents, columns = ['principal component 1', 'principal component 2',\n",
    "                                                                 'principal component 3','principal component 4',\n",
    "                                                                 'principal component 5','principal component 6'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "principalDf.iloc[:,:3].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eig_values = pca.explained_variance_\n",
    "eig_vectors = pca.components_\n",
    "print(eig_values)\n",
    "print(eig_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine the first pricipal component\n",
    "eig_vectors[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine the components\n",
    "pc1 = pca.components_[0]\n",
    "pc2 = pca.components_[1]\n",
    "# the .components attribute shows principal axes in feature space, representing the directions of maximum variance in the data. \n",
    "#The components are sorted by explained_variance_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(pc1)\n",
    "print(pc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the loadings\n",
    "structure_loading_1 = pc1* np.sqrt(eig_values[0])\n",
    "str_loading_1 = pd.Series(structure_loading_1, index=features)\n",
    "str_loading_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "str_loading_1.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = np.arange(6)\n",
    "plt.bar(index, pca.explained_variance_ratio_)\n",
    "plt.title('Scree plot for PCA')\n",
    "plt.xlabel('Num of components')\n",
    "plt.ylabel('proportion of explained variance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting screeplots \n",
    "#print(pca.explained_variance_ratio_)\n",
    "#print(pca.explained_variance_)\n",
    "\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('explained variance ratio')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using numpy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can also use numpy to solve for this aa\n",
    "import numpy as np \n",
    "corr_mat = pd.DataFrame(x).corr()\n",
    "eigenvalues, eigenvectors = np.linalg.eig(corr_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "PCA is one of the most versatile algorithm used not only in research in data science but also social sciences, natural sciences and others. It allows us to further examine the relationship between different variables in our dataset, and the importance of each variable. Some of the practical applications of PCA are:\n",
    "- [Facial Recognition](https://en.wikipedia.org/wiki/Eigenface)\n",
    "- General purpose dimensionality reduction \n",
    "- Clinical psychology -- distinguishing patients with schizophrenia from healthy patients [link](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2566788/)\n",
    "- Data Visualization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
