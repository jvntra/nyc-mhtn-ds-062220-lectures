{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Natural Language Processing and NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agenda today:\n",
    "- Text analytics and NLP\n",
    "- Pre-Prosessing for NLP \n",
    "    - Tokenization\n",
    "    - Stopwords removal\n",
    "    - Lexicon normalization: lemmatization and stemming\n",
    "- Feature Engineering for NLP\n",
    "    - Bag-of-Words\n",
    "    - Term frequency-Inverse Document Frequency (tf-idf)\n",
    "- Text Classification\n",
    "    - Use features from text to detect satire\n",
    "- Next Steps/Project Ideas :)\n",
    "    - Document clustering: Latent Dirichlet Allocation (LDA), Latent Semantic Analysis(LSA) \n",
    "    - Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I. Text Analytics and NLP\n",
    "NLP allows computers to interact with text data in a structured and sensible way. In this section, we will discuss some steps and approaches to common text data analytic procedures. In other words, with NLP, computers are taught to understand human language, its meaning and sentiments. Some of the applications of natural language processing are:\n",
    "- Chatbots \n",
    "- Classifying documents \n",
    "- Speech recognition and audio processing \n",
    "\n",
    "In this section, we will introduce you to the preprocessing steps, feature engineering, and other steps you need to take in order to format text data for machine learning tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overview of NLP process \n",
    "<img src=\"attachment:Screen%20Shot%202019-03-22%20at%207.35.58%20AM.png\" style=\"width:500px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II. Pre-Prosessing for NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install nltk\n",
    "#!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "review = 'From the beginning of the movie, it gives the feeling the director is trying to portray something, what I mean to say that instead of the story dictating the style in which the movie should be made, he has gone in the opposite way, he had a type of move that he wanted to make, and wrote a story to suite it. And he has failed in it very badly. I guess he was trying to make a stylish movie. Any way I think this movie is a total waste of time and effort. In the credit of the director, he knows the media that he is working with, what I am trying to say is I have seen worst movies than this. Here at least the director knows to maintain the continuity in the movie. And the actors also have given a decent performance.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sklearn\r\n",
      "Requirement already satisfied: scikit-learn in /Users/lotuschild132/anaconda/envs/learn-env/lib/python3.6/site-packages (from sklearn) (0.21.2)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/lotuschild132/anaconda/envs/learn-env/lib/python3.6/site-packages (from scikit-learn->sklearn) (0.13.2)\r\n",
      "Requirement already satisfied: numpy>=1.11.0 in /Users/lotuschild132/anaconda/envs/learn-env/lib/python3.6/site-packages (from scikit-learn->sklearn) (1.16.5)\r\n",
      "Requirement already satisfied: scipy>=0.17.0 in /Users/lotuschild132/anaconda/envs/learn-env/lib/python3.6/site-packages (from scikit-learn->sklearn) (1.3.1)\r\n",
      "Installing collected packages: sklearn\r\n",
      "Successfully installed sklearn-0.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/lotuschild132/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/lotuschild132/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/lotuschild132/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "#from sklearn.model_selection import train_test_split\n",
    "from matplotlib import cm\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization \n",
    "Tokenization is the process of splitting documents into units of observations. We usually represent the tokens as __n-gram__, where n represent the consecutive words occuring in a document. In the case of unigram (one word token), the sentence \"David works here\" can be tokenized into?\n",
    "\n",
    "\"David\", \"works\", \"here\"\n",
    "\"David works\", \"works here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'From the beginning of the movie, it gives the feeling the director is trying to portray something, what I mean to say that instead of the story dictating the style in which the movie should be made, he has gone in the opposite way, he had a type of move that he wanted to make, and wrote a story to suite it. And he has failed in it very badly. I guess he was trying to make a stylish movie. Any way I think this movie is a total waste of time and effort. In the credit of the director, he knows the media that he is working with, what I am trying to say is I have seen worst movies than this. Here at least the director knows to maintain the continuity in the movie. And the actors also have given a decent performance.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "\n",
    "tokenized_review = tokenizer.tokenize(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RegexpTokenizer is a tokenizer that splits a string using a regular expression, which matches either the tokens or the separators between tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['From', 'the', 'beginning', 'of', 'the', 'movie', 'it', 'gives', 'the', 'feeling', 'the', 'director', 'is', 'trying', 'to', 'portray', 'something', 'what', 'I', 'mean', 'to', 'say', 'that', 'instead', 'of', 'the', 'story', 'dictating', 'the', 'style', 'in', 'which', 'the', 'movie', 'should', 'be', 'made', 'he', 'has', 'gone', 'in', 'the', 'opposite', 'way', 'he', 'had', 'a', 'type', 'of', 'move', 'that', 'he', 'wanted', 'to', 'make', 'and', 'wrote', 'a', 'story', 'to', 'suite', 'it', 'And', 'he', 'has', 'failed', 'in', 'it', 'very', 'badly', 'I', 'guess', 'he', 'was', 'trying', 'to', 'make', 'a', 'stylish', 'movie', 'Any', 'way', 'I', 'think', 'this', 'movie', 'is', 'a', 'total', 'waste', 'of', 'time', 'and', 'effort', 'In', 'the', 'credit', 'of', 'the', 'director', 'he', 'knows', 'the', 'media', 'that', 'he', 'is', 'working', 'with', 'what', 'I', 'am', 'trying', 'to', 'say', 'is', 'I', 'have', 'seen', 'worst', 'movies', 'than', 'this', 'Here', 'at', 'least', 'the', 'director', 'knows', 'to', 'maintain', 'the', 'continuity', 'in', 'the', 'movie', 'And', 'the', 'actors', 'also', 'have', 'given', 'a', 'decent', 'performance']\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 78 samples and 145 outcomes>\n"
     ]
    }
   ],
   "source": [
    "fdist = FreqDist(tokenized_review)\n",
    "print(fdist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "fdist.plot(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are the words very informative? If we were to extract informatiom based on this frequency distribution of the most common words in tis graph, are we going to get some helpful information or not? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hers', 'such', 'me', 'here', 'the', 'on', \"it's\", 'when', 'd', 'no', 'whom', \"she's\", 'their', 'own', 'how', 'at', 'just', 'all', 'than', 'we', 'down', \"hadn't\", \"mustn't\", 'other', 'an', 'out', \"mightn't\", 'doing', 'she', 'don', 'which', 'myself', 'won', 'this', 'had', 'only', 'for', 'before', \"don't\", \"you're\", 'not', 'him', 'ain', \"couldn't\", 'above', 'its', 'off', 'now', 'while', 'himself', \"shan't\", 'yours', 'itself', 'a', 'after', 've', 'am', 'with', 'yourself', 'because', 'against', 'into', 'couldn', 'wasn', \"didn't\", \"hasn't\", 'who', 'mustn', 'any', 'm', 'ma', 'if', 'below', 't', 'you', 'shan', \"that'll\", 'll', \"doesn't\", 'where', 'they', 'about', 'does', 'through', 'under', 'in', 'them', 'more', 'having', 'are', 'once', 'should', 'wouldn', 'ourselves', 'from', 'is', 'nor', 'over', 'theirs', 'some', 'were', 'hasn', 'isn', 'haven', \"aren't\", \"isn't\", 'do', 'up', 'these', \"haven't\", 'weren', \"wouldn't\", 'that', 'being', 'themselves', 'too', 'and', 'both', 'herself', 'i', 'your', 'there', 'most', 'has', 'again', 'doesn', 'yourselves', 's', 'very', 'did', 'mightn', 'be', 'o', 'shouldn', 'can', 'same', \"should've\", 'his', 'of', 'ours', 'between', 'needn', 'what', 'each', 're', \"you'd\", 'was', 'during', 'aren', 'few', 'her', 'it', 'but', 'or', 'as', 'by', 'didn', 'to', \"weren't\", 'those', 'until', 'further', 'been', 'will', 'then', \"you've\", \"needn't\", \"you'll\", 'our', \"shouldn't\", 'hadn', \"wasn't\", \"won't\", 'have', 'why', 'my', 'y', 'so', 'he'}\n"
     ]
    }
   ],
   "source": [
    "stop_words=set(stopwords.words(\"english\"))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filterd Sentence: ['From', 'beginning', 'movie', 'gives', 'feeling', 'director', 'trying', 'portray', 'something', 'I', 'mean', 'say', 'instead', 'story', 'dictating', 'style', 'movie', 'made', 'gone', 'opposite', 'way', 'type', 'move', 'wanted', 'make', 'wrote', 'story', 'suite', 'And', 'failed', 'badly', 'I', 'guess', 'trying', 'make', 'stylish', 'movie', 'Any', 'way', 'I', 'think', 'movie', 'total', 'waste', 'time', 'effort', 'In', 'credit', 'director', 'knows', 'media', 'working', 'I', 'trying', 'say', 'I', 'seen', 'worst', 'movies', 'Here', 'least', 'director', 'knows', 'maintain', 'continuity', 'movie', 'And', 'actors', 'also', 'given', 'decent', 'performance']\n"
     ]
    }
   ],
   "source": [
    "filtered_review=[]\n",
    "for w in tokenized_review:\n",
    "    if w not in stop_words:\n",
    "        filtered_review.append(w)\n",
    "print(\"Filterd Sentence:\",filtered_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145\n",
      "72\n"
     ]
    }
   ],
   "source": [
    "print(len(tokenized_review))\n",
    "print(len(filtered_review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fdist = FreqDist(filtered_review)\n",
    "plt.figure(figsize=(10,10))\n",
    "fdist.plot(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have removed semantically meaningless words, can we possibly have other types of problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lexicon Normalization \n",
    "Aside from stopwords, a different type of noise can arise in NLP. For example, collect, collection, collected, and collecting are all similar words. Using stemming and lemmatization would reduce all variations of the same word to the root version of all its derivations. \n",
    "\n",
    "###### Stemming \n",
    "Stemming allows us to remove different variations of the same word. For example, collect, collection and collecting will all be reduced to the same single word collect.\n",
    "- Stemming is the process of reducing inflection in words to their root forms, such as mapping a group of words to the same stem even if the stem itself is not a valid word in the Language.\n",
    " \n",
    "- Stems are created by removing the suffixes or prefixes used with a word.\n",
    "<img src=\"attachment:Screen%20Shot%202019-08-13%20at%2010.45.02%20AM.png\" width=400;>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['From', 'begin', 'movi', 'give', 'feel', 'director', 'tri', 'portray', 'someth', 'I', 'mean', 'say', 'instead', 'stori', 'dictat', 'style', 'movi', 'made', 'gone', 'opposit', 'way', 'type', 'move', 'want', 'make', 'wrote', 'stori', 'suit', 'And', 'fail', 'badli', 'I', 'guess', 'tri', 'make', 'stylish', 'movi', 'Ani', 'way', 'I', 'think', 'movi', 'total', 'wast', 'time', 'effort', 'In', 'credit', 'director', 'know', 'media', 'work', 'I', 'tri', 'say', 'I', 'seen', 'worst', 'movi', 'Here', 'least', 'director', 'know', 'maintain', 'continu', 'movi', 'And', 'actor', 'also', 'given', 'decent', 'perform']\n"
     ]
    }
   ],
   "source": [
    "ps = PorterStemmer()\n",
    "\n",
    "stemmed_review=[]\n",
    "for w in filtered_review:\n",
    "    stemmed_review.append(ps.stem(w))\n",
    "\n",
    "print(stemmed_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAExCAYAAABoA4mxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12b5c5fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fdist = FreqDist(stemmed_review)\n",
    "fdist.plot(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatization\n",
    "The only difference between lemmatization and stemming is that lemmatization returns real words. For example, instead of returning \"movi\" like Porter stemmer would, \"movie\" will be returned by the lemmatizer.\n",
    "\n",
    "- Unlike Stemming, Lemmatization reduces the inflected words properly ensuring that the root word belongs to the language. \n",
    "\n",
    "- In Lemmatization, the root word is called Lemma. \n",
    "\n",
    "- A lemma (plural lemmas or lemmata) is the canonical form, dictionary form, or citation form of a set of words.\n",
    "\n",
    "<img src=\"attachment:Screen%20Shot%202019-08-13%20at%2010.45.08%20AM.png\" width=400;>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer \n",
    "  \n",
    "lemmatizer = WordNetLemmatizer() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movies: movie\n",
      "collecting: collecting\n",
      "collection: collection\n",
      "collections: collection\n"
     ]
    }
   ],
   "source": [
    "print(\"movies:\", lemmatizer.lemmatize(\"movies\")) \n",
    "print(\"collecting:\", lemmatizer.lemmatize(\"collecting\")) \n",
    "print(\"collection:\", lemmatizer.lemmatize(\"collection\")) \n",
    "print(\"collections:\", lemmatizer.lemmatize(\"collections\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movies: movi\n",
      "collecting : collect\n"
     ]
    }
   ],
   "source": [
    "# comparing it with stemming \n",
    "print(\"movies:\", ps.stem(\"movies\")) \n",
    "print(\"collecting :\", ps.stem(\"collecting\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['From', 'beginning', 'movie', 'give', 'feeling', 'director', 'trying', 'portray', 'something', 'I', 'mean', 'say', 'instead', 'story', 'dictating', 'style', 'movie', 'made', 'gone', 'opposite', 'way', 'type', 'move', 'wanted', 'make', 'wrote', 'story', 'suite', 'And', 'failed', 'badly', 'I', 'guess', 'trying', 'make', 'stylish', 'movie', 'Any', 'way', 'I', 'think', 'movie', 'total', 'waste', 'time', 'effort', 'In', 'credit', 'director', 'know', 'medium', 'working', 'I', 'trying', 'say', 'I', 'seen', 'worst', 'movie', 'Here', 'least', 'director', 'know', 'maintain', 'continuity', 'movie', 'And', 'actor', 'also', 'given', 'decent', 'performance']\n"
     ]
    }
   ],
   "source": [
    "# we can also lemmatize our original reviews\n",
    "lemmatized_review=[]\n",
    "for w in filtered_review:\n",
    "    lemmatized_review.append(lemmatizer.lemmatize(w))\n",
    "\n",
    "print(lemmatized_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part III. Feature Engineering for NLP \n",
    "The machine learning algorithms we have encountered so far represent features as the variables that take on different value for each observation. For example, we represent individual with distinct education level, income, and such. However, in NLP, features are represented in very different way. In order to pass text data to machine learning algorithm and perform classification, we need to represent the features in a sensible way. One such method is called Bag-of-words (BoW). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bag-of-words model, or BoW for short, is a way of extracting features from text for use in modeling. A bag-of-words is a representation of text that describes the occurrence of words within a document. It involves two things:\n",
    "\n",
    "- A vocabulary of known words.\n",
    "- A measure of the presence of known words.\n",
    "\n",
    "It is called a “bag” of words, because any information about the order or structure of words in the document is discarded. The model is only concerned with whether known words occur in the document, not where in the document. The intuition behind BoW is that a document is similar to another if they have similar contents. Bag of Words method can be represented as **Document Term Matrix**, or Term Document Matrix, in which each column is an unique vocabulary, each observation is a document. For example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Document 1: \"I love dogs\"\n",
    "- Document 2: \"I love cats\"\n",
    "- Document 3: \"I love all animals\"\n",
    "- Document 4: \"I hate dogs\"\n",
    "\n",
    "\n",
    "Can be represented as:\n",
    "<img src=\"attachment:Screen%20Shot%202019-03-22%20at%208.16.32%20AM.png\" style=\"width:600px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>all</th>\n",
       "      <th>animals</th>\n",
       "      <th>cats</th>\n",
       "      <th>dogs</th>\n",
       "      <th>love</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   all  animals  cats  dogs  love\n",
       "0    0        0     0     1     1\n",
       "1    0        0     1     0     1\n",
       "2    1        1     0     0     1"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# implementing it in python\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# Convert a collection of text documents to a matrix of token counts\n",
    "\n",
    "docs = ['i love dogs','i love cats','i love all animals']\n",
    "vec = CountVectorizer(stop_words=None)\n",
    "X = vec.fit_transform(docs)\n",
    "\n",
    "\n",
    "df = pd.DataFrame(X.toarray(), columns = vec.get_feature_names())\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF \n",
    "There are many schemas for determining the values of each entry in a document term matrix, and one of the most common schema is called the TF-IDF -- term frequency-inverse document frequency. Essentially, tf-idf *normalizes* the raw count of the document term matrix. And it represents how important a word is in the given document. \n",
    "\n",
    "- TF (Term Frequency)\n",
    "term frequency is simply the frequency of words in a document, and it can be represented as the number of times a term shows up in a document. \n",
    "\n",
    "- IDF (inverse document frequency)\n",
    "IDF represents the measure of how much information the word provides, i.e., if it's common or rare across all documents. It is the logarithmically scaled inverse fraction of the documents that contain the word (obtained by dividing the total number of documents by the number of documents containing the term, and then taking the logarithm of that quotient):\n",
    "\n",
    "$$idf(w) = log (\\frac{number of documents}{num of documents containing W})$$\n",
    "\n",
    "tf-idf is the product of term frequency and inverse document frequency, or tf * idf. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6/10 Acting, not great but some good acting.&lt;b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Devil Hunter gained notoriety for the fact tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>`Stanley and Iris' is a heart warming film abo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This may not be a memorable classic, but it is...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review\n",
       "0  6/10 Acting, not great but some good acting.<b...\n",
       "1  Devil Hunter gained notoriety for the fact tha...\n",
       "2  `Stanley and Iris' is a heart warming film abo...\n",
       "3  This may not be a memorable classic, but it is..."
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's implement it \n",
    "import pandas as pd\n",
    "review_1 = \"6/10 Acting, not great but some good acting.<br /><br />4/10 Director, makes some stupid decisions for this film.<br /><br />2/10 Writer, story makes no sense at all and has huge amount of flaws.<br /><br />4/10 Overall score for this movie.<br /><br />Don't waste your time with this film, it's not worth it. I gave 4 for this movie and it may be too much. Characters are so over exaggerated than they can ever be in real life and some pretty unexplainable stuff happens 'storywise', not in good way. Because of the style this film has been filmed you get bored after 30 minutes (too many special effects: slow motions and camera shakes and fast forwards). It's always good that movie uses music to make the story go smooth but there's too many tracks in this one. In the first hour there is almost 50/50 dialogs and musics\"\n",
    "review_2 = \"Devil Hunter gained notoriety for the fact that it's on the DPP 'Video Nasty' list, but it really needn't have been. Many films on the list where there for God (and DPP) only known reasons, and while this isn't the tamest of the bunch; there isn't a lot here that warrants banning...which is a shame because I never would have sat through it where it not for the fact that it's on 'the shopping list'. The plot actually gives the film a decent base - or at least more of a decent base than most cannibal films - and it follows an actress who is kidnapped and dragged off into the Amazon jungle. A hunter is then hired to find her, but along the way he has to brave the natives, lead by a man who calls himself 'The Devil' (hence the title). The film basically just plods along for eighty five minutes and there really aren't many scenes of interest. It's a real shame that Jess Franco ended up making films like this because the man clearly has talent; as seen by films such as The Diabolical Dr Z, Venus in Furs, Faceless and She Kills in Ecstasy, but unfortunately his good films are just gems amongst heaps of crap and Devil Hunter is very much a part of the crap. I saw this film purely because I want to be able to say I've seen everything on the DPP's list (just two more to go!), and I'm guessing that's why most other people who have seen it, saw it. But if you're not on the lookout for Nasties; there really is no reason to bother with this one.\"\n",
    "review_3 = \"`Stanley and Iris' is a heart warming film about two people who find each other and help one another overcome their problems in life. Stanley's life is difficult, because he never learned to read or write. Iris is a widower with two teenage children working in a bakery where she meets Stanley. She decides to teach Stanley how to read at her home in her spare time. Over time they become romantically involved. After Stanley learns to read, he goes off to a good job in Chicago, only to return to Iris and ask her to marry him.<br /><br />It's a really good film without nudity, violence, or profanity, that which is rare in today's films. A good film all round. <br /><br />\"\n",
    "review_4 = \"This may not be a memorable classic, but it is a touching romance with an important theme that stresses the importance of literacy in modern society and the devastating career and life consequences for any unfortunate individual lacking this vital skill.<br /><br />The story revolves around Iris, a widow who becomes acquainted with a fellow employee at her factory job, an illiterate cafeteria worker named Stanley. Iris discovers that Stanley is unable to read, and after he loses his job, she gives him reading lessons at home in her kitchen. Of course, as you might predict, the two, although initially wary of involvement, develop feelings for each other...<br /><br />Jane Fonda competently plays Iris, a woman with problems of her own, coping with a job lacking prospects, two teenage children (one pregnant), an unemployed sister and her abusive husband. However, Robert DeNiro is of course brilliant in his endearing portrayal of the intelligent and resourceful, but illiterate, Stanley, bringing a dignity to the role that commands respect. They aren't your typical charming young yuppie couple, as generally depicted in on screen romances, but an ordinary working class, middle aged pair with pretty down to earth struggles.<br /><br />I won't give the ending away, but it's a lovely, heartwarming romance and a personal look into the troubling issue of adult illiteracy, albeit from the perspective of a fictional character.\"\n",
    "df = pd.DataFrame([review_1,review_2,review_3, review_4],columns = ['review'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "cv = CountVectorizer(lowercase=True,stop_words='english',ngram_range = (1,1),tokenizer = token.tokenize)\n",
    "text_counts= cv.fit_transform(df['review'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(text_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x275 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 308 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "PandasError",
     "evalue": "DataFrame constructor not properly called!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPandasError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-a4537b97486b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_counts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/lotuschild132/anaconda/lib/python3.4/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    300\u001b[0m                                          copy=False)\n\u001b[1;32m    301\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mPandasError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'DataFrame constructor not properly called!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0mNDFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmgr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfastpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPandasError\u001b[0m: DataFrame constructor not properly called!"
     ]
    }
   ],
   "source": [
    "pd.DataFrame(text_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10</th>\n",
       "      <th>2</th>\n",
       "      <th>30</th>\n",
       "      <th>4</th>\n",
       "      <th>50</th>\n",
       "      <th>6</th>\n",
       "      <th>able</th>\n",
       "      <th>abusive</th>\n",
       "      <th>acquainted</th>\n",
       "      <th>acting</th>\n",
       "      <th>...</th>\n",
       "      <th>woman</th>\n",
       "      <th>won</th>\n",
       "      <th>worker</th>\n",
       "      <th>working</th>\n",
       "      <th>worth</th>\n",
       "      <th>write</th>\n",
       "      <th>writer</th>\n",
       "      <th>young</th>\n",
       "      <th>yuppie</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 275 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   10  2  30  4  50  6  able  abusive  acquainted  acting ...  woman  won  \\\n",
       "0   4  1   1  3   2  1     0        0           0       2 ...      0    0   \n",
       "1   0  0   0  0   0  0     1        0           0       0 ...      0    0   \n",
       "2   0  0   0  0   0  0     0        0           0       0 ...      0    0   \n",
       "3   0  0   0  0   0  0     0        1           1       0 ...      1    1   \n",
       "\n",
       "   worker  working  worth  write  writer  young  yuppie  z  \n",
       "0       0        0      1      0       1      0       0  0  \n",
       "1       0        0      0      0       0      0       0  1  \n",
       "2       0        1      0      1       0      0       0  0  \n",
       "3       1        1      0      0       0      1       1  0  \n",
       "\n",
       "[4 rows x 275 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(text_counts.todense(),columns = cv.get_feature_names())\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['10', '2', '30', '4', '50', '6', 'able', 'abusive', 'acquainted',\n",
       "       'acting',\n",
       "       ...\n",
       "       'woman', 'won', 'worker', 'working', 'worth', 'write', 'writer',\n",
       "       'young', 'yuppie', 'z'],\n",
       "      dtype='object', length=275)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6/10 Acting, not great but some good acting.&lt;b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Devil Hunter gained notoriety for the fact tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>`Stanley and Iris' is a heart warming film abo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This may not be a memorable classic, but it is...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review\n",
       "0  6/10 Acting, not great but some good acting.<b...\n",
       "1  Devil Hunter gained notoriety for the fact tha...\n",
       "2  `Stanley and Iris' is a heart warming film abo...\n",
       "3  This may not be a memorable classic, but it is..."
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use tfidf vectorizer instead\n",
    "review_1 = \"6/10 Acting, not great but some good acting.<br /><br />4/10 Director, makes some stupid decisions for this film.<br /><br />2/10 Writer, story makes no sense at all and has huge amount of flaws.<br /><br />4/10 Overall score for this movie.<br /><br />Don't waste your time with this film, it's not worth it. I gave 4 for this movie and it may be too much. Characters are so over exaggerated than they can ever be in real life and some pretty unexplainable stuff happens 'storywise', not in good way. Because of the style this film has been filmed you get bored after 30 minutes (too many special effects: slow motions and camera shakes and fast forwards). It's always good that movie uses music to make the story go smooth but there's too many tracks in this one. In the first hour there is almost 50/50 dialogs and musics\"\n",
    "review_2 = \"Devil Hunter gained notoriety for the fact that it's on the DPP 'Video Nasty' list, but it really needn't have been. Many films on the list where there for God (and DPP) only known reasons, and while this isn't the tamest of the bunch; there isn't a lot here that warrants banning...which is a shame because I never would have sat through it where it not for the fact that it's on 'the shopping list'. The plot actually gives the film a decent base - or at least more of a decent base than most cannibal films - and it follows an actress who is kidnapped and dragged off into the Amazon jungle. A hunter is then hired to find her, but along the way he has to brave the natives, lead by a man who calls himself 'The Devil' (hence the title). The film basically just plods along for eighty five minutes and there really aren't many scenes of interest. It's a real shame that Jess Franco ended up making films like this because the man clearly has talent; as seen by films such as The Diabolical Dr Z, Venus in Furs, Faceless and She Kills in Ecstasy, but unfortunately his good films are just gems amongst heaps of crap and Devil Hunter is very much a part of the crap. I saw this film purely because I want to be able to say I've seen everything on the DPP's list (just two more to go!), and I'm guessing that's why most other people who have seen it, saw it. But if you're not on the lookout for Nasties; there really is no reason to bother with this one.\"\n",
    "review_3 = \"`Stanley and Iris' is a heart warming film about two people who find each other and help one another overcome their problems in life. Stanley's life is difficult, because he never learned to read or write. Iris is a widower with two teenage children working in a bakery where she meets Stanley. She decides to teach Stanley how to read at her home in her spare time. Over time they become romantically involved. After Stanley learns to read, he goes off to a good job in Chicago, only to return to Iris and ask her to marry him.<br /><br />It's a really good film without nudity, violence, or profanity, that which is rare in today's films. A good film all round. <br /><br />\"\n",
    "review_4 = \"This may not be a memorable classic, but it is a touching romance with an important theme that stresses the importance of literacy in modern society and the devastating career and life consequences for any unfortunate individual lacking this vital skill.<br /><br />The story revolves around Iris, a widow who becomes acquainted with a fellow employee at her factory job, an illiterate cafeteria worker named Stanley. Iris discovers that Stanley is unable to read, and after he loses his job, she gives him reading lessons at home in her kitchen. Of course, as you might predict, the two, although initially wary of involvement, develop feelings for each other...<br /><br />Jane Fonda competently plays Iris, a woman with problems of her own, coping with a job lacking prospects, two teenage children (one pregnant), an unemployed sister and her abusive husband. However, Robert DeNiro is of course brilliant in his endearing portrayal of the intelligent and resourceful, but illiterate, Stanley, bringing a dignity to the role that commands respect. They aren't your typical charming young yuppie couple, as generally depicted in on screen romances, but an ordinary working class, middle aged pair with pretty down to earth struggles.<br /><br />I won't give the ending away, but it's a lovely, heartwarming romance and a personal look into the troubling issue of adult illiteracy, albeit from the perspective of a fictional character.\"\n",
    "df = pd.DataFrame([review_1,review_2,review_3, review_4],columns = ['review'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.06926739,  0.06926739,  0.13853478,  0.06926739,  0.0361466 ,\n",
       "        0.06926739,  0.06926739,  0.0361466 ,  0.06926739,  0.10922241,\n",
       "        0.06926739,  0.05461121,  0.06926739,  0.0361466 ,  0.06926739,\n",
       "        0.06926739,  0.0361466 ,  0.06926739,  0.06926739,  0.06926739,\n",
       "        0.06926739,  0.06926739,  0.06926739,  0.06926739,  0.06926739,\n",
       "        0.06926739,  0.10922241,  0.05461121,  0.06926739,  0.04421248,\n",
       "        0.06926739,  0.06926739,  0.04421248,  0.06926739,  0.05461121,\n",
       "        0.06926739,  0.13263743,  0.04421248,  0.05461121,  0.06926739,\n",
       "        0.06926739,  0.06926739,  0.06926739,  0.05461121,  0.04421248,\n",
       "        0.05461121,  0.14458639,  0.06926739,  0.06926739,  0.04421248,\n",
       "        0.05461121,  0.06926739,  0.05461121,  0.06926739,  0.05461121,\n",
       "        0.06926739,  0.05461121,  0.20780217,  0.08842496,  0.05461121,\n",
       "        0.06926739,  0.06926739,  0.14458639,  0.0361466 ,  0.05461121,\n",
       "        0.05461121,  0.06926739,  0.06926739,  0.20780217,  0.06926739,\n",
       "        0.06926739,  0.06926739,  0.08842496,  0.06926739,  0.06926739,\n",
       "        0.10922241,  0.21687959,  0.05461121,  0.0361466 ,  0.06926739,\n",
       "        0.05461121,  0.10922241,  0.06926739,  0.13263743,  0.26527487,\n",
       "        0.13263743,  0.06926739,  0.06926739,  0.13853478,  0.06926739,\n",
       "        0.35369982,  0.13263743,  0.20780217,  0.08842496,  0.06926739,\n",
       "        0.13263743,  0.13853478,  0.27706956,  0.04284744,  0.04284744,\n",
       "        0.04284744,  0.04284744,  0.04284744,  0.04284744,  0.03378142,\n",
       "        0.02734897,  0.04284744,  0.04284744,  0.02734897,  0.04284744,\n",
       "        0.04284744,  0.04284744,  0.04284744,  0.04284744,  0.04284744,\n",
       "        0.08569489,  0.04284744,  0.04284744,  0.08569489,  0.04284744,\n",
       "        0.04284744,  0.04284744,  0.03378142,  0.04284744,  0.04284744,\n",
       "        0.04284744,  0.02734897,  0.04284744,  0.04284744,  0.04284744,\n",
       "        0.04284744,  0.04284744,  0.04284744,  0.12854233,  0.06756283,\n",
       "        0.04284744,  0.04284744,  0.04284744,  0.04284744,  0.04284744,\n",
       "        0.04284744,  0.04284744,  0.04284744,  0.04284744,  0.04284744,\n",
       "        0.03378142,  0.04284744,  0.04284744,  0.04284744,  0.12854233,\n",
       "        0.04284744,  0.04284744,  0.04284744,  0.04284744,  0.04284744,\n",
       "        0.08569489,  0.08569489,  0.04284744,  0.04284744,  0.04284744,\n",
       "        0.02734897,  0.08569489,  0.02734897,  0.03378142,  0.04284744,\n",
       "        0.04284744,  0.04284744,  0.04284744,  0.03378142,  0.03378142,\n",
       "        0.04284744,  0.04284744,  0.0820469 ,  0.04284744,  0.03378142,\n",
       "        0.04284744,  0.04284744,  0.08569489,  0.08569489,  0.04284744,\n",
       "        0.03378142,  0.08569489,  0.08569489,  0.03378142,  0.04284744,\n",
       "        0.04284744,  0.04284744,  0.04284744,  0.04284744,  0.04284744,\n",
       "        0.03378142,  0.08569489,  0.03378142,  0.04284744,  0.04284744,\n",
       "        0.04284744,  0.04284744,  0.04284744,  0.04284744,  0.08569489,\n",
       "        0.04284744,  0.04284744,  0.04284744,  0.03378142,  0.04284744,\n",
       "        0.06756283,  0.16890708,  0.12854233,  0.04284744,  0.10134425,\n",
       "        0.17138977,  0.04284744,  0.04284744,  0.12854233,  0.16890708,\n",
       "        0.08569489,  0.04284744,  0.04284744,  0.12854233,  0.12854233,\n",
       "        0.11179787,  0.02235957,  0.13512566,  0.03378142,  0.13415744,\n",
       "        0.11179787,  0.06756283,  0.03378142,  0.02734897,  0.03378142,\n",
       "        0.54697935,  0.0820469 ,  0.03378142,  0.03378142,  0.04471915,\n",
       "        0.03378142,  0.03378142,  0.03378142,  0.02734897,  0.20123616,\n",
       "        0.02235957,  0.13674484,  0.06756283,  0.17887658,  0.02235957,\n",
       "        0.03378142,  0.0820469 ,  0.10939587,  0.13674484,  0.02734897,\n",
       "        0.10939587,  0.05469794,  0.08606896,  0.08606896,  0.08606896,\n",
       "        0.08606896,  0.08606896,  0.08606896,  0.08606896,  0.06785775,\n",
       "        0.08606896,  0.08606896,  0.08606896,  0.08606896,  0.06785775,\n",
       "        0.08606896,  0.08606896,  0.08606896,  0.08606896,  0.08606896,\n",
       "        0.08606896,  0.06785775,  0.08606896,  0.08606896,  0.08606896,\n",
       "        0.08606896,  0.08606896,  0.06785775,  0.06785775,  0.06785775,\n",
       "        0.08606896,  0.08606896,  0.20357326,  0.08606896,  0.08606896,\n",
       "        0.06785775,  0.08606896,  0.08606896,  0.08606896,  0.08606896,\n",
       "        0.06785775,  0.08606896,  0.08606896,  0.08606896,  0.20357326,\n",
       "        0.33928877,  0.06785775,  0.0549367 ,  0.1098734 ,  0.1098734 ,\n",
       "        0.1098734 ,  0.1648101 ,  0.06785775,  0.06785775,  0.0549367 ,\n",
       "        0.13571551,  0.06785775,  0.06785775,  0.06785775,  0.06785775,\n",
       "        0.06785775,  0.06785775,  0.17965741,  0.04491435,  0.35931482,\n",
       "        0.04491435,  0.0549367 ,  0.0549367 ,  0.1098734 ,  0.22457176,\n",
       "        0.0549367 ,  0.06785775,  0.04491435,  0.04491435,  0.13571551,\n",
       "        0.13474306,  0.06785775,  0.04491435,  0.1648101 ,  0.2197468 ,\n",
       "        0.1648101 ,  0.05901167,  0.05901167,  0.05901167,  0.05901167,\n",
       "        0.05901167,  0.05901167,  0.05901167,  0.05901167,  0.05901167,\n",
       "        0.05901167,  0.05901167,  0.05901167,  0.05901167,  0.05901167,\n",
       "        0.05901167,  0.05901167,  0.05901167,  0.05901167,  0.05901167,\n",
       "        0.05901167,  0.05901167,  0.05901167,  0.05901167,  0.05901167,\n",
       "        0.05901167,  0.05901167,  0.05901167,  0.05901167,  0.05901167,\n",
       "        0.05901167,  0.05901167,  0.05901167,  0.05901167,  0.05901167,\n",
       "        0.05901167,  0.05901167,  0.05901167,  0.05901167,  0.05901167,\n",
       "        0.05901167,  0.05901167,  0.05901167,  0.05901167,  0.05901167,\n",
       "        0.05901167,  0.05901167,  0.05901167,  0.05901167,  0.05901167,\n",
       "        0.05901167,  0.05901167,  0.05901167,  0.05901167,  0.05901167,\n",
       "        0.05901167,  0.05901167,  0.05901167,  0.05901167,  0.05901167,\n",
       "        0.05901167,  0.05901167,  0.05901167,  0.05901167,  0.05901167,\n",
       "        0.05901167,  0.05901167,  0.05901167,  0.05901167,  0.11802335,\n",
       "        0.05901167,  0.05901167,  0.05901167,  0.05901167,  0.05901167,\n",
       "        0.05901167,  0.05901167,  0.05901167,  0.05901167,  0.11802335,\n",
       "        0.05901167,  0.05901167,  0.05901167,  0.05901167,  0.05901167,\n",
       "        0.05901167,  0.05901167,  0.05901167,  0.05901167,  0.05901167,\n",
       "        0.11802335,  0.05901167,  0.05901167,  0.05901167,  0.05901167,\n",
       "        0.05901167,  0.05901167,  0.05901167,  0.05901167,  0.05901167,\n",
       "        0.05901167,  0.05901167,  0.05901167,  0.05901167,  0.11802335,\n",
       "        0.05901167,  0.05901167,  0.05901167,  0.04652548,  0.13957644,\n",
       "        0.04652548,  0.04652548,  0.04652548,  0.04652548,  0.04652548,\n",
       "        0.04652548,  0.04652548,  0.13957644,  0.13957644,  0.03766639,\n",
       "        0.07533277,  0.09305096,  0.03766639,  0.09305096,  0.04652548,\n",
       "        0.03766639,  0.15066554,  0.04652548,  0.03766639,  0.18610192,\n",
       "        0.04652548,  0.04652548,  0.09238422,  0.03079474,  0.09238422,\n",
       "        0.09238422,  0.03766639,  0.03766639,  0.33899748,  0.04652548,\n",
       "        0.03766639,  0.12317896,  0.03766639,  0.03766639,  0.04652548,\n",
       "        0.06158948,  0.15397369,  0.04652548,  0.30133109,  0.18476843,\n",
       "        0.06158948,  0.04652548,  0.07533277,  0.07533277,  0.22599832,\n",
       "        0.15066554,  0.03766639])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tf=TfidfVectorizer()\n",
    "text_tf= tf.fit_transform(df['review'])\n",
    "text_tf.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine what type of object text_tf is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what does this function do?\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "\n",
    "def fn_tdm_df(docs, xColNames = None, **kwargs):\n",
    "    ''' create a term document matrix as pandas DataFrame\n",
    "    with **kwargs you can pass arguments of CountVectorizer\n",
    "    if xColNames is given the dataframe gets columns Names'''\n",
    "\n",
    "    #initialize the  vectorizer\n",
    "    vectorizer = CountVectorizer(**kwargs)\n",
    "    x1 = vectorizer.fit_transform(docs)\n",
    "    #create dataFrame\n",
    "    df = pd.DataFrame(x1.toarray().transpose(), index = vectorizer.get_feature_names())\n",
    "\n",
    "    if xColNames is not None:\n",
    "        df.columns = xColNames\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10</th>\n",
       "      <th>30</th>\n",
       "      <th>50</th>\n",
       "      <th>able</th>\n",
       "      <th>about</th>\n",
       "      <th>abusive</th>\n",
       "      <th>acquainted</th>\n",
       "      <th>acting</th>\n",
       "      <th>actress</th>\n",
       "      <th>actually</th>\n",
       "      <th>...</th>\n",
       "      <th>worker</th>\n",
       "      <th>working</th>\n",
       "      <th>worth</th>\n",
       "      <th>would</th>\n",
       "      <th>write</th>\n",
       "      <th>writer</th>\n",
       "      <th>you</th>\n",
       "      <th>young</th>\n",
       "      <th>your</th>\n",
       "      <th>yuppie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 371 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   10  30  50  able  about  abusive  acquainted  acting  actress  actually  \\\n",
       "0   4   1   2     0      0        0           0       2        0         0   \n",
       "1   0   0   0     1      0        0           0       0        1         1   \n",
       "2   0   0   0     0      1        0           0       0        0         0   \n",
       "3   0   0   0     0      0        1           1       0        0         0   \n",
       "\n",
       "    ...    worker  working  worth  would  write  writer  you  young  your  \\\n",
       "0   ...         0        0      1      0      0       1    1      0     1   \n",
       "1   ...         0        0      0      1      0       0    1      0     0   \n",
       "2   ...         0        1      0      0      1       0    0      0     0   \n",
       "3   ...         1        1      0      0      0       0    1      1     1   \n",
       "\n",
       "   yuppie  \n",
       "0       0  \n",
       "1       0  \n",
       "2       0  \n",
       "3       1  \n",
       "\n",
       "[4 rows x 371 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fn_tdm_df(df['review']).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what about this function?\n",
    "def fn_tdm_tfidf(docs, xColNames = None, **kwargs):\n",
    "    ''' create a term document matrix as pandas DataFrame\n",
    "    with **kwargs you can pass arguments of CountVectorizer\n",
    "    if xColNames is given the dataframe gets columns Names'''\n",
    "\n",
    "    #initialize the  vectorizer\n",
    "    tf = TfidfVectorizer(**kwargs)\n",
    "    x1 = tf.fit_transform(docs)\n",
    "    #create dataFrame\n",
    "    df = pd.DataFrame(x1.toarray().transpose(), index = tf.get_feature_names())\n",
    "\n",
    "    if xColNames is not None:\n",
    "        df.columns = xColNames\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10</th>\n",
       "      <th>30</th>\n",
       "      <th>50</th>\n",
       "      <th>able</th>\n",
       "      <th>about</th>\n",
       "      <th>abusive</th>\n",
       "      <th>acquainted</th>\n",
       "      <th>acting</th>\n",
       "      <th>actress</th>\n",
       "      <th>actually</th>\n",
       "      <th>...</th>\n",
       "      <th>worker</th>\n",
       "      <th>working</th>\n",
       "      <th>worth</th>\n",
       "      <th>would</th>\n",
       "      <th>write</th>\n",
       "      <th>writer</th>\n",
       "      <th>you</th>\n",
       "      <th>young</th>\n",
       "      <th>your</th>\n",
       "      <th>yuppie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.27707</td>\n",
       "      <td>0.069267</td>\n",
       "      <td>0.138535</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.138535</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.069267</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.069267</td>\n",
       "      <td>0.044212</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.054611</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042847</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042847</td>\n",
       "      <td>0.042847</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042847</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027349</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.086069</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.067858</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.086069</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.059012</td>\n",
       "      <td>0.059012</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059012</td>\n",
       "      <td>0.046525</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037666</td>\n",
       "      <td>0.059012</td>\n",
       "      <td>0.046525</td>\n",
       "      <td>0.059012</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 371 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        10        30        50      able     about   abusive  acquainted  \\\n",
       "0  0.27707  0.069267  0.138535  0.000000  0.000000  0.000000    0.000000   \n",
       "1  0.00000  0.000000  0.000000  0.042847  0.000000  0.000000    0.000000   \n",
       "2  0.00000  0.000000  0.000000  0.000000  0.086069  0.000000    0.000000   \n",
       "3  0.00000  0.000000  0.000000  0.000000  0.000000  0.059012    0.059012   \n",
       "\n",
       "     acting   actress  actually    ...       worker   working     worth  \\\n",
       "0  0.138535  0.000000  0.000000    ...     0.000000  0.000000  0.069267   \n",
       "1  0.000000  0.042847  0.042847    ...     0.000000  0.000000  0.000000   \n",
       "2  0.000000  0.000000  0.000000    ...     0.000000  0.067858  0.000000   \n",
       "3  0.000000  0.000000  0.000000    ...     0.059012  0.046525  0.000000   \n",
       "\n",
       "      would     write    writer       you     young      your    yuppie  \n",
       "0  0.000000  0.000000  0.069267  0.044212  0.000000  0.054611  0.000000  \n",
       "1  0.042847  0.000000  0.000000  0.027349  0.000000  0.000000  0.000000  \n",
       "2  0.000000  0.086069  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "3  0.000000  0.000000  0.000000  0.037666  0.059012  0.046525  0.059012  \n",
       "\n",
       "[4 rows x 371 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fn_tdm_tfidf(df['review']).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification\n",
    "Now that you have a great basic understanding of feature engineering and preprosessing in NLP, we can move on to text classification using Naive Bayes and other classification algorithm. We can treat the engineered dataframes like any other dataframes that you have worked with before. \n",
    "\n",
    "Now, how would Naive Bayes treat the classification problem? What is the prior, posterior, and evidence, in the calculation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Preprocessing & Cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('nlp_classification.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Noting that the resignation of James Mattis as...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Desperate to unwind after months of nonstop wo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nearly halfway through his presidential term, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Attempting to make amends for gross abuses of ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Decrying the Senate’s resolution blaming the c...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                body  target\n",
       "0  Noting that the resignation of James Mattis as...       1\n",
       "1  Desperate to unwind after months of nonstop wo...       1\n",
       "2  Nearly halfway through his presidential term, ...       1\n",
       "3  Attempting to make amends for gross abuses of ...       1\n",
       "4  Decrying the Senate’s resolution blaming the c...       1"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Noting that the resignation of James Mattis as Secretary of Defense marked the ouster of the third top administration official in less than three weeks, a worried populace told reporters Friday that it was unsure how many former Trump staffers it could safely reabsorb. “Jesus, we can’t just take back these assholes all at once—we need time to process one before we get the next,” said 53-year-old Gregory Birch of Naperville, IL echoing the concerns of 323 million Americans in also noting that the country was only now truly beginning to reintegrate former national security advisor Michael Flynn. “This is just not sustainable. I’d say we can handle maybe one or two more former members of Trump’s inner circle over the remainder of the year, but that’s it. This country has its limits.” The U.S. populace confirmed that they could not handle all of these pieces of shit trying to rejoin society at once.'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.body[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df['body']\n",
    "target = df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string, re\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "## setting stopwords and punctuations\n",
    "sw_list = stopwords.words('english')\n",
    "sw_list += list(string.punctuation)\n",
    "sw_list += [\"''\", '\"\"', '...', '``', '’', '“', '’', '”', '‘', '‘', '©',\n",
    "            'said', 'one', 'com', 'satirewire', '-', '–', '—', 'satirewire.com']\n",
    "sw_set = set(sw_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can define a function that removes stopwords \n",
    "def process_article(article):\n",
    "    tokens = nltk.word_tokenize(article)\n",
    "    stopwords_removed = [token.lower() for token in tokens if token.lower() not in sw_set]\n",
    "    return stopwords_removed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying the above function to our data/features \n",
    "processed_data = list(map(process_article, data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29165"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_vocab = set()\n",
    "for comment in processed_data:\n",
    "    total_vocab.update(comment)\n",
    "len(total_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a list with all lemmatized outputs\n",
    "lemmatized_output = []\n",
    "\n",
    "for listy in processed_data:\n",
    "    lemmed = ' '.join([lemmatizer.lemmatize(w) for w in listy])\n",
    "    lemmatized_output.append(lemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_lem = lemmatized_output\n",
    "# [' '.join(d) for d in processed_data]\n",
    "\n",
    "y_lem = target\n",
    "# X[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Corpus Statistics and Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\r\n",
      "PackageNotFoundError: Package not found: 'sklearn' Package 'sklearn' is not installed in /Users/lotuschild132/anaconda/envs/learn-env\r\n",
      "\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!conda update sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_test_split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-3fbe564f50c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train_lem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_lem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_lem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_lem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_lem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_lem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtfidf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtfidf_data_train_lem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_lem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtfidf_data_test_lem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_lem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_test_split' is not defined"
     ]
    }
   ],
   "source": [
    "X_train_lem, X_test_lem, y_train_lem, y_test_lem = train_test_split(X_lem, y_lem, test_size=0.20, random_state=1)\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "tfidf_data_train_lem = tfidf.fit_transform(X_train_lem)\n",
    "tfidf_data_test_lem = tfidf.transform(X_test_lem)\n",
    "\n",
    "tfidf_data_train_lem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "non_zero_cols = tfidf_data_train_lem.nnz / float(tfidf_data_train_lem.shape[0])\n",
    "print(\"Average Number of Non-Zero Elements in Vectorized Articles: {}\".format(non_zero_cols))\n",
    "\n",
    "percent_sparse = 1 - (non_zero_cols / float(tfidf_data_train_lem.shape[1]))\n",
    "print('Percentage of columns containing ZERO: {}'.format(percent_sparse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Most Frequent Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_freq_satire = df[df['target']==1]\n",
    "df_freq_not_satire = df[df['target']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sat = df_freq_satire['body']\n",
    "data_not_sat = df_freq_not_satire['body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pros_satire = list(map(process_article, data_sat))\n",
    "pros_not_satire = list(map(process_article, data_not_sat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_vocab_sat = set()\n",
    "for comment in pros_satire:\n",
    "    total_vocab_sat.update(comment)\n",
    "len(total_vocab_sat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_vocab_NOT_sat = set()\n",
    "for comment in pros_not_satire:\n",
    "    total_vocab_NOT_sat.update(comment)\n",
    "len(total_vocab_NOT_sat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pros_satire[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing:\n",
    "# We will use these again later on\n",
    "# FORMAT:  flat_list = [item for sublist in l for item in sublist]\n",
    "\n",
    "flat_satire = [item for sublist in pros_satire for item in sublist]\n",
    "flat_not_satire = [item for sublist in pros_not_satire for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "satire_freq = FreqDist(flat_satire)\n",
    "not_satire_freq = FreqDist(flat_not_satire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 20 satire words:\n",
    "\n",
    "satire_freq.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 20 non-satire words:\n",
    "\n",
    "not_satire_freq.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalized word frequencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "satire_total_word_count = sum(satire_freq.values())\n",
    "satire_top_25 = satire_freq.most_common(25)\n",
    "print(\"Word \\t\\t Normalized Frequency\")\n",
    "print()\n",
    "for word in satire_top_25:\n",
    "    normalized_frequency = word[1]/satire_total_word_count\n",
    "    print(\"{} \\t\\t {:.4}\".format(word[0], normalized_frequency))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_satire_total_word_count = sum(not_satire_freq.values())\n",
    "not_satire_top_25 = not_satire_freq.most_common(25)\n",
    "print(\"Word \\t\\t Normalized Frequency\")\n",
    "print()\n",
    "for word in not_satire_top_25:\n",
    "    normalized_frequency = word[1]/not_satire_total_word_count\n",
    "    print(\"{} \\t\\t {:.4}\".format(word[0], normalized_frequency))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's visualize it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create counts of satire and not satire with values and words\n",
    "satire_bar_counts = [x[1] for x in satire_freq.most_common(25)]\n",
    "satire_bar_words = [x[0] for x in satire_freq.most_common(25)]\n",
    "\n",
    "not_satire_bar_counts = [x[1] for x in not_satire_freq.most_common(25)]\n",
    "not_satire_bar_words = [x[0] for x in not_satire_freq.most_common(25)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the color of our bar graphs\n",
    "color = cm.viridis_r(np.linspace(.4,.8, 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_figure = plt.figure(figsize=(16,4))\n",
    "\n",
    "ax = new_figure.add_subplot(121)\n",
    "ax2 = new_figure.add_subplot(122)\n",
    "\n",
    "# Generate a line plot on first axes\n",
    "ax.bar(satire_bar_words, satire_bar_counts, color=color)\n",
    "# ax.plot(colormap='PRGn')\n",
    "\n",
    "# Draw a scatter plot on 2nd axes\n",
    "ax2.bar(not_satire_bar_words, not_satire_bar_counts, color=color )\n",
    "\n",
    "ax.title.set_text('Satire')\n",
    "ax2.title.set_text('Not Satire')\n",
    "\n",
    "for ax in new_figure.axes:\n",
    "    plt.sca(ax)\n",
    "    plt.xticks(rotation=60)\n",
    "\n",
    "plt.tight_layout(pad=0)\n",
    "\n",
    "# plt.savefig('word count bar graphs.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting our data into a dictionary\n",
    "# FORMAT:  dictionary = dict(zip(keys, values))\n",
    "# !pip install wordcloud\n",
    "from wordcloud import WordCloud\n",
    "satire_dictionary = dict(zip(satire_bar_words, satire_bar_counts))\n",
    "not_satire_dictionary = dict(zip(not_satire_bar_words, not_satire_bar_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the satire dictionary:\n",
    "\n",
    "# satire_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the word cloud:\n",
    "\n",
    "wordcloud = WordCloud(colormap='Spectral').generate_from_frequencies(satire_dictionary)\n",
    "\n",
    "# Display the generated image w/ matplotlib:\n",
    "\n",
    "plt.figure(figsize=(10,10), facecolor='k')\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad=0)\n",
    "\n",
    "# Uncomment the next line if you want to save your image:\n",
    "# plt.savefig('satire_wordcloud.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(colormap='Spectral').generate_from_frequencies(not_satire_dictionary)\n",
    "\n",
    "plt.figure(figsize=(10,10), facecolor='k')\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad=0)\n",
    "# plt.savefig('not_satire_wordcloud.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's classify!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_classifier_lem = RandomForestClassifier(n_estimators=100, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_classifier_lem.fit(tfidf_data_train_lem, y_train_lem)\n",
    "\n",
    "rf_test_preds_lem = rf_classifier_lem.predict(tfidf_data_test_lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_acc_score_lem = accuracy_score(y_test_lem, rf_test_preds_lem)\n",
    "rf_f1_score_lem = f1_score(y_test_lem, rf_test_preds_lem)\n",
    "print('Random Forest with Lemmatization Features')\n",
    "print(\"Testing Accuracy: {:.4}\".format(rf_acc_score_lem))\n",
    "print()\n",
    "print(\"F1 Score: {:.4}\".format(rf_f1_score_lem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "\n",
    "mat = confusion_matrix(y_test_lem, rf_test_preds_lem)\n",
    "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,\n",
    "            xticklabels=['Satire', 'Not_Satire'], yticklabels=['Satire', 'Not_Satire'])\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion - how would Naive Bayes treat the features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your turn - use naive bayes to classify satire vs not satire?\n",
    "\n",
    "# what kind of naive bayes are we going to use?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions and Next Steps\n",
    "- Learning foundations of NLP allows us represent our language in a way that computers understand\n",
    "- We can use the machine learning algorithms that we already learned to classify text documents\n",
    "- However, there are still disadvantages to represent language this way\n",
    "- Topic Modeling \n",
    "- Word embeddings: word2vec and doc2vec\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
